<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LeoHung.self()</title><link href="http://leohung.net/" rel="alternate"></link><link href="http://leohung.net/feeds/blogs.atom.xml" rel="self"></link><id>http://leohung.net/</id><updated>2015-05-13T01:30:00+02:00</updated><entry><title>Large-scale Distributed Machine Learning: MapReduce, Data Graph, and Parameter Server</title><link href="http://leohung.net/large-scale-distributed-machine-learning-mapreduce-data-graph-and-parameter-server.html" rel="alternate"></link><updated>2015-05-13T01:30:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2015-05-13:large-scale-distributed-machine-learning-mapreduce-data-graph-and-parameter-server.html</id><summary type="html">&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Big machine learning is hard because (1) the amount of traning data is huge (2) the number of parameters is large. This note summarizes three paradigms of distributed architecture for large-scale machine learning problem: MapReduce, Data Graph, and Parameter Server.&lt;/p&gt;
&lt;h2&gt;Why should we build distributed system for large-scale machine Learning?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Large-scale&lt;/em&gt; has two meanings: (1) the amount of training data is huge which can not easily fit in a machine. (2) the number of parameters is too large to be stored and manipulated by a single machine. Therefore, the large-scale machine learning and data mining usually cannot be solved by a single machine. In stead, developers need to construct their machine learning algorithms into a distributed system.&lt;/p&gt;
&lt;h2&gt;MapReduce Approach&lt;/h2&gt;
&lt;p&gt;MapReduce [9] is a well-known distributed computing model on clusters, and Apache Hadoop [10] is the best known open source counterpart for MapReduce. In general, MapReduce contains two phases: map phase and reduce phase. In map phase, the provided map program should process the input chunks into key-value pairs. After map phase, the key-value pairs will be shuffled into the specific reduce nodes according to the hash value of the key. The reducers will receive the list of values with the corresponding key, perform its reduce function, and store the output into a distributed file system.&lt;/p&gt;
&lt;p&gt;Many research tried to fit the existing machine learning algorithms into MapReduce framework. They need to transform original algorithms into map functions and reduce functions in multiple iterations. Mahout [1] is a well-known open source for large-scale machine learning mainly based on Hadoop.&lt;/p&gt;
&lt;p&gt;Hadoop is designed to store the output of one map-reduce phase into disk; however, many machine learning algorithms need iterative calculation until the termination condition is achieved (e.g. the algorithms in gradient decent fashion). Multiple iteration computing on Hadoop can cause huge amount of I/O time and deteriorate the performance.&lt;/p&gt;
&lt;p&gt;To tackle with this problem, Spark [2] provides Resilient Distributed Datasets (RDD) for in-memory computation, which can cache the output of one MapReduce iteration into memory and load it from memory in the next iteration, so RDD can improve performance by avoiding disk I/O. According to their experiments, Spark can have 20x speedup over Hadoop in logistic regression and k-means algorithm. MLlib is a popular machine learning framework based on Spark.&lt;/p&gt;
&lt;p&gt;Another problem for MapReduce approach mentioned by Low et al. (2012) [3] is that the framework may generate huge amount of duplicated messages in the map phase and increase network cost. Taking Pagerank as an example, the high degree nodes need to send multiple duplication of its Pagerank value to its neighbors in map phase. The network traffic may reduce the performance of the algorithms.&lt;/p&gt;
&lt;h2&gt;Data Graph Approach&lt;/h2&gt;
&lt;p&gt;The data graph approach means to model the computation relationship between the variables in the machine learning algorithm into a data graph: the nodes are the variables and the edges are the connection of message passing.&lt;/p&gt;
&lt;p&gt;Pregel [4] is a distributed graph computing framework. The programs in Pregel are executed in continuous supersteps. In each superstep S, the framework call the defined functions on the vertices in parallel, and the vertex can read the message sent to it in S-1, and also send messages to other vertices which can read the message in S+1 superstep. Pregel is well suited in graph algorithms like Pagerank, shortest path, and clustering. Although Google does not release the source code of Pregel to public, but it has an open source counterpart called Apache Giraph.&lt;/p&gt;
&lt;p&gt;GraphLab [3] is an another distributed graph computing framework. Unlike Pregel, GraphLab does not have synchronous supersteps for message passing. Instead, it uses vertex functions in asynchronous fashion to specify what kind of computation is performed and which vertices will run in the next round, and it uses three different consistency models (vertex consistency, edge consistency, and full consistency) to assure serializablility. GraphLab is improved in the next version PowerGraph [5], which can deal with high degree vertices by splitting them into multiple vertices. Besides, GraphLab has an variant called GraphChi [6], which focuses on how to implement graph computation logic in a single machine.&lt;/p&gt;
&lt;h2&gt;Parameter Server Approach&lt;/h2&gt;
&lt;p&gt;Parameter Server [7] is a servers-workers style distributed computing framework. The servers storing global variables in distributed hashtable fashion, supporting pull, push, and aggregation operations of global variables for the workers. The training data is partitioned and stored in the workers where the main computation happens. Taking distributed logistic regression as an example, one calculation cycle is as follows: the workers pull the global weights from the servers. Each worker computes its local gradient descent value according its data, and then push back to the servers for aggregation. After the aggregation, the workers pull the global weights again.&lt;/p&gt;
&lt;p&gt;Petuum [8] also follows Parameter Server architecture. They developed a more specific communication mechanism called Stale Synchronous Parallel(SSP) to mitigate the workers from waiting. Users can set the parameter staleness S. A fastest worker need to wait if it is faster than the slowest worker S iterations. SSP can reduce the network traffic because workers do not need to spend too much time waiting for other worker updates.&lt;/p&gt;
&lt;p&gt;[1] Mahout. http://mahout.apache.org. (n.d.). Mahout. http://mahout.apache.org. Retrieved February 22, 2015, from http://mahout.apache.org/&lt;/p&gt;
&lt;p&gt;[2] Zaharia, M., Chowdhury, M., Das, T., Dave, A., Ma, J., McCauly, M., et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. Nsdi, 15–28.&lt;/p&gt;
&lt;p&gt;[3] Low, Y., Gonzalez, J. E., Kyrola, A., Bickson, D., Guestrin, C., &amp;amp; Hellerstein, J. M. (2014). GraphLab: A New Framework For Parallel Machine Learning. CoRR Abs/1204.6078, cs.LG.&lt;/p&gt;
&lt;p&gt;[4] Malewicz, G., Austern, M. H., Bik, A. J. C., Dehnert, J. C., Horn, I., Leiser, N., &amp;amp; Czajkowski, G. (2010). Pregel: a system for large-scale graph processing. Sigmod, 135–146. http://doi.org/10.1145/1807167.1807184&lt;/p&gt;
&lt;p&gt;[5] Gonzalez, J. E., Low, Y., Gu, H., Bickson, D., &amp;amp; Guestrin, C. (2012). PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. Osdi, 17–30.&lt;/p&gt;
&lt;p&gt;[6] Kyrola, A., Blelloch, G. E., &amp;amp; Guestrin, C. (2012). GraphChi: Large-Scale Graph Computation on Just a PC. Osdi, 31–46.&lt;/p&gt;
&lt;p&gt;[7] Li, M., Andersen, D. G., Smola, A. J., &amp;amp; Yu, K. (2014). Communication Efficient Distributed Machine Learning with the Parameter Server. Nips, 19–27.&lt;/p&gt;
&lt;p&gt;[8] Dai, W., Wei, J., Zheng, X., Kim, J. K., Lee, S., Yin, J., et al. (2013). Petuum: A Framework for Iterative-Convergent Distributed ML. CoRR Abs/1204.6078, stat.ML.&lt;/p&gt;
&lt;p&gt;[9] Dean, J., &amp;amp; Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1). http://doi.org/10.1145/1327452.1327492&lt;/p&gt;
&lt;p&gt;[10] Hadoop. http://hadoop.apache.org. (n.d.). Hadoop. http://hadoop.apache.org. Retrieved February 22, 2015, from http://hadoop.apache.org/&lt;/p&gt;</summary><category term="machine learning"></category><category term="distributed system"></category><category term="big-ml"></category></entry><entry><title>"NetworkX Network: Discovering the core of company network in Taiwan by using NetworkX. (用NetworkX找出台灣公司網絡核心) @ PyCon TW 2014"</title><link href="http://leohung.net/networkx-network-discovering-the-core-of-company-network-in-taiwan-by-using-networkx-yong-networkxzhao-chu-tai-wan-gong-si-wang-luo-he-xin-pycon-tw-2014.html" rel="alternate"></link><updated>2014-05-17T01:30:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2014-05-17:networkx-network-discovering-the-core-of-company-network-in-taiwan-by-using-networkx-yong-networkxzhao-chu-tai-wan-gong-si-wang-luo-he-xin-pycon-tw-2014.html</id><summary type="html">&lt;h1&gt;Slide&lt;/h1&gt;
&lt;p&gt;&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/34775422" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="https://www.slideshare.net/AskusHong/pycon2014" title="NetworkX Network: 用NetworkX找出台灣公司網絡核心 by Leo Hung @PyCon TW 2014" target="_blank"&gt;NetworkX Network: 用NetworkX找出台灣公司網絡核心 by Leo Hung @PyCon TW 2014&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="http://www.slideshare.net/AskusHong" target="_blank"&gt;SanChan Hong&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/LeoHung/pycon2014_tw_company_core_network"&gt;Github Link&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Closeness" src="/images/closeness_50.png" /&gt;&lt;/p&gt;</summary><category term="networkx"></category><category term="python"></category><category term="pycon_2014"></category><category term="pycon_tw"></category></entry><entry><title>"Teaching Elasticsearch reading Chinese in 1 Minutes "</title><link href="http://leohung.net/teaching-elasticsearch-reading-chinese-in-1-minutes.html" rel="alternate"></link><updated>2014-04-26T19:07:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2014-04-26:teaching-elasticsearch-reading-chinese-in-1-minutes.html</id><summary type="html">&lt;h1&gt;Teaching Elasticsearch reading Chinese in 1 Minutes&lt;/h1&gt;
&lt;p&gt;Elasticsearch(es) is a powerful open source solution for searching, which is serving in wikipedia currently; however, Chinese users is not so lucky with es, becasue the default analyzer of es cannot deal with Chinese well, which will tokenzie Chinese sentences into individual characters, rather than terms, causing large amount of noise in retrieved results.&lt;/p&gt;
&lt;p&gt;To enhance the performance of retrieval, setting the analyzer is necessary; however, there is no quick and effective plug-in for traditional Chinese. IKAnalyzer, developed by China community, may be the best plug-in for Chinese Elasticsearch engineers to serve the job, but IKAnalyzer cannot be set easily.&lt;/p&gt;
&lt;p&gt;If you just want a quick solution, cjk analyzer is the best solution. The algorithm of cjk analyzer is easy to be understood: tokenizing sentences to  bi-gram terms. For example, "我愛吃飯" will be transformed into "我愛", "愛吃", "吃飯". The analyzer is naive but powerful to enhance es performance.&lt;/p&gt;
&lt;p&gt;The below is the setting syntax :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PUT &amp;quot;index name&amp;quot;
&amp;quot;settings&amp;quot;: {
     &amp;quot;analysis&amp;quot;:{
        &amp;quot;analyzer&amp;quot;:{
            &amp;quot;default&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;cjk&amp;quot;}
        }
     }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Beware, the setting only work when the index is empty. If there are data in the index, do not forget to re-index it.&lt;/p&gt;
&lt;h1&gt;一分鐘搞定Elasticsearch中文設定&lt;/h1&gt;
&lt;p&gt;Elasticsearch(es)是一套強大的搜尋引擎套件，目前維基百科正是使用es作為搜尋套件。但是，es的預設設定對中文使用者並不是很友善，因為es預設的分析器(Analyzer)不能處理好中文的斷詞(tokenization)，會把中文詞切成一個字一個字，而非以詞組為單位，使得搜尋效果變差，時常會搜尋到過多雜訊。&lt;/p&gt;
&lt;p&gt;因此，需要設定好es的斷詞設定，才能強化中文搜尋的結果。但可惜的是，繁體中文並沒有已經被最佳化的套件可以無痛與es接軌。目前已知比較好的方案，像是中國社群(誰?)發展出 IKanalyzer ，但需要花一點時間做複雜的設定。&lt;/p&gt;
&lt;p&gt;如果只是短期內需要一個堪用的做法，可以試試看cjk分析器。cjk分析器的原理很簡單，也就只是做 Bi-Gram，把讀進來的句子，兩個兩個切成一個詞。例如：「我愛吃飯」，就變成「我愛, 愛吃, 吃飯」。簡單的設定，就可以強化es的搜尋結果。&lt;/p&gt;
&lt;p&gt;設定的語法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PUT &amp;quot;index name&amp;quot;
&amp;quot;settings&amp;quot;: {
     &amp;quot;analysis&amp;quot;:{
        &amp;quot;analyzer&amp;quot;:{
            &amp;quot;default&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;cjk&amp;quot;}
        }
     }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;請注意，settings必須在index剛建立之初設定才有效。如果index裡面已經有資料，請reindex你的資料庫。&lt;/p&gt;</summary><category term="Elasticsearch"></category><category term="Chinese"></category><category term="Analyzer"></category></entry><entry><title>"Semi-Supervised Hashing for Scalable Image Retrieval"</title><link href="http://leohung.net/semi-supervised-hashing-for-scalable-image-retrieval.html" rel="alternate"></link><updated>2013-09-08T00:40:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:semi-supervised-hashing-for-scalable-image-retrieval.html</id><summary type="html">&lt;p&gt;J. Wang et al, "Semi-Supervised Hashing for Scalable Image Retrieval," CVPR, 2010.&lt;/p&gt;
&lt;p&gt;The following figures and formula are all copied from this paper.&lt;/p&gt;
&lt;h2&gt;Novelty&lt;/h2&gt;
&lt;p&gt;Unsupervised learning method, like LSH, would ignore the relationship of labels; however, supervised learning method modified from LSH, like RBM or SH, would cost time to compute and require much training data to avoid overfitting. To coping with these problems, the authors introduce a semi-supervised learning method: SSH.&lt;/p&gt;
&lt;h2&gt;Technical Summarization&lt;/h2&gt;
&lt;p&gt;SSH(Semi-Supervised Hashing) is based on two idea: (1) distribute the similar label images to the closer hash (2) make each hash bucket balanced. Assume that $latex M $ is the images pairs set in which have same label, and $latex C $ is the images pairs set where have different labels. The following objective function is main to the first criterion: for pairs with same label, maximize the probability such that they have fall into the closer buckets; for pairs with different, minimize such probability.&lt;/p&gt;
&lt;p&gt;&lt;img alt="03different" src="/images/ammai/03/different.png" /&gt;&lt;/p&gt;
&lt;p&gt;Besides, they add a regularization to constraint such that each bucket size should be balanced: hash functions should be orthogonal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="balance_constraint" src="/images/ammai/03/balance_constraint.png" /&gt;&lt;/p&gt;
&lt;p&gt;Nevertheless, the balancing constraints are hard, so it is replaced by a "soft" constraint, instead of requiring orthogonality, maximizing the variance of mapping space.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;$latex \sum_k{E[ { \left \| h_k(x) - \mu \right \|}^{2} ]}$&lt;/p&gt;

&lt;p&gt;The following figure is the comparison between state-of-the-art methods and SSH in large scale data. They demonstrate that SSH with no-orthogonal performs better than other algorithms&lt;/p&gt;
&lt;p&gt;&lt;img alt="w4_result" src="/images/ammai/03/w4_result.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My comments&lt;/h2&gt;
&lt;p&gt;Just as the authors claim, SSH can maintain the semantic meanings consistency, unlike LSH method, and do not require too much time like supervised learning methods like RBM and SH. Due to SSH is a semi-supervised method, SSH must have label data to train model; however, in reality, the labels are often missing or incomplete. I am curious how to conquer this problem, which is not the main point solved by the study, but I think it is important because it will occur in practice.&lt;/p&gt;</summary></entry><entry><title>"Mairal et al. Online dictionary learning for sparse coding"</title><link href="http://leohung.net/mairal-et-al-online-dictionary-learning-for-sparse-coding.html" rel="alternate"></link><updated>2013-09-08T00:35:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:mairal-et-al-online-dictionary-learning-for-sparse-coding.html</id><summary type="html">&lt;p&gt;Mairal et al. Online dictionary learning for sparse coding. ICML 2009.&lt;/p&gt;
&lt;p&gt;The following figures are all from this paper.&lt;/p&gt;
&lt;p&gt;The paper goal is to propose a online learning method to learn sparse coding dictionary, the basis set which can represent specific data through linear combination, for large scale data.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;&lt;img alt="online_learning_algorithm" src="/images/ammai/03/online_learning_algorithm.png" /&gt;&lt;/p&gt;
&lt;p&gt;The algorithm is showed above. Assuming the training set composed of i.i.d. samples of a distribution, the method is minimizing the cost function, the difference between linear combination of basis set and real data, by stochastic gradient discent to find decomposing coefficient $latex \alpha $ and dictionary $latex D$ iteratively.&lt;/p&gt;
&lt;p&gt;&lt;img alt="online_updating_algorithm" src="/images/ammai/03/online_updating_algorithm.png" /&gt;&lt;/p&gt;
&lt;p&gt;Their algorithm above for updating the dictionary uses block-coordinate descent.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;The work is solid because they present not only the experiment result  but also the sophisticated theoretical proof showing the convergence of the online learning method; however, the assumption, the training set composed of i.i.d. samples of a distribution, may be suspicious because it is not alway satisfied in real world.&lt;/p&gt;</summary></entry><entry><title>"Aggregating local descriptors into a compact image representation"</title><link href="http://leohung.net/aggregating-local-descriptors-into-a-compact-image-representation.html" rel="alternate"></link><updated>2013-09-08T00:28:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:aggregating-local-descriptors-into-a-compact-image-representation.html</id><summary type="html">&lt;p&gt;Herve Jegou, et al., Aggregating local descriptors into a compact image representation, Proc. IEEE CVPR'10
The goal of the work is reducing the computing time and the memory usage without lossing too much accuracy in image retrieval for large scale data. This work consists of two parts: VLAD and vector encoding.&lt;/p&gt;
&lt;h2&gt;VLAD&lt;/h2&gt;
&lt;p&gt;Inspired by Fisher kernel, they introduce VLAD(vector of locally aggregated descriptors) to represent) to represent an image. The VLAD is processed as follows.&lt;/p&gt;
&lt;p&gt;First, train a visual words codebook&lt;/p&gt;
&lt;p&gt;&lt;img alt="c_set" src="/images/ammai/03/c_set.gif" /&gt;&lt;/p&gt;
&lt;p&gt;by K-means, where $latex c_i $ means the i'th centroid, and $latex c_i$ is a dimension d sift vector.&lt;/p&gt;
&lt;p&gt;Second, for the sift vector of each image $latex x$, whose dimension = d, aggregate the difference between $latex x$ and its nearest centroid $latex c_i $ to generate $latex v_{i,j}$ as bellow, where i means the number of centroid and j means the number of the component of sift vector. $latex v's $  dimension $latex D = k * d$.&lt;/p&gt;
&lt;p&gt;Finally, the vector v is subsequently L2-normalized by $latex v := v/||v||^2 $.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vlad" src="/images/ammai/03/vlad.png" /&gt;&lt;/p&gt;
&lt;p&gt;Approximating nearest neighbors&lt;/p&gt;
&lt;h2&gt;Indexation-aware dimensionality reduction&lt;/h2&gt;
&lt;p&gt;To make nearest neighbor search more efficiently by approximating, they proposed ADC approach, which quantize each sub-vector. They also find that VLAD is sparse and structured, which is appropriate for PCA to reduce dimensionality. To consider that PCA transformed vector's variance is not equal, it can employ Householder matrix to balance variance; besides, they find that random orthogonal matrix also performs well according to the experiment result.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;The paper contribution is proposing VLAD to represent  image and using ADC approach and PCA to compress data.  The experiment shows that the VLAD they proposed performs better than "bag of features" method. This approach can make image retrieval more efficiently and more space economically.&lt;/p&gt;</summary></entry><entry><title>"Efficient visual search of videos cast as text retrieval"</title><link href="http://leohung.net/efficient-visual-search-of-videos-cast-as-text-retrieval.html" rel="alternate"></link><updated>2013-09-08T00:23:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:efficient-visual-search-of-videos-cast-as-text-retrieval.html</id><summary type="html">&lt;p&gt;"Efficient visual search of videos cast as text retrieval," J. Sivic, and A. Zisserman, IEEE TPAMI, 2009
Paper Authors: Josef Sivic and Andrew Zisserman The following figures are all from this paper.&lt;/p&gt;
&lt;p&gt;&lt;img alt="outline" src="/images/ammai/03/outline.png" /&gt;&lt;/p&gt;
&lt;p&gt;J. Sivic, and A. Zisserman proposed this method which analogizes searching in video to in text corpora: representing a visual region as a visual word such that it can employ text retrieval methods like inverted index, tf-idf...etc. The diagram not only performs well but also saves time efficiently. The system flowchart shows as above.&lt;/p&gt;
&lt;h2&gt;Visual words&lt;/h2&gt;
&lt;p&gt;The first thing to do is to convert image regions into visual words, which are symbols can represent key image regions like word terms' effect in articles. To do so, use two affine covariant regions detecting method: Shape Adapted (SA) and Maximally Stable (MS)  to find significant regions, describe them with SIFT descriptors, cluster descriptor vectors with K-means using Mahalanbis distance as below, and transfer vectors into "visual words" according to the clustering result. Visual words not only can be represent image regions but also makes image retrieval more efficiently through inverted index method.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mahalanobis-distance" src="/images/ammai/03/Mahalanobis-distance.png" /&gt;&lt;/p&gt;
&lt;h2&gt;tf-idf weighted and Stop list&lt;/h2&gt;
&lt;p&gt;Borrowing the concept of tf-idf in text retrieval, the query vectors and retrieved frame vectors are weighted by the inverse document. And for the most frequent visual words in all most all image are suppressed, just like what Stop word list effects.&lt;/p&gt;
&lt;h2&gt;Spatial consistency&lt;/h2&gt;
&lt;p&gt;It is a accurate match not only hit one point but also the points surrounding the point are matched. Each region which also matches within the search areas, defined by the 15 nearest spatial neighbours of each match, casts a vote for that frame. Matches with no support are rejected. The ﬁnal relevance score of the frame is determined by summing the spatial consistency votes, and adding the frequency score $latex sim( v_q , v_d ) $, the consine similarity between $latex v_q$ and $latex v_d$.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;This work shows how cross-fields research can be possible and why it may be more useful: the well-developed techniques in other filed can be applied if we analogizes the problem to the other appropriately. However, there are still data specific attributes, just like spatial consistence, which should be processed more sophisticatedly. .&lt;/p&gt;</summary></entry><entry><title>"Distinctive Image Features from Scale-Invariant Keypoints"</title><link href="http://leohung.net/distinctive-image-features-from-scale-invariant-keypoints.html" rel="alternate"></link><updated>2013-09-08T00:07:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:distinctive-image-features-from-scale-invariant-keypoints.html</id><summary type="html">&lt;p&gt;"Distinctive Image Features from Scale-Invariant Keypoints," Lowe, IJCV, 2004&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mslab.csie.ntu.edu.tw/~askus/ammai/blog/wp-content/uploads/2012/03/螢幕快照-2012-03-13-下午11.11.08.png"&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper author: David G. Lowe The following figures are all from this paper.&lt;/p&gt;
&lt;h2&gt;Novelties, contributions, assumption&lt;/h2&gt;
&lt;p&gt;This work introduce a set of promising affine invariant features, which can be used to identified specific object varied with background, illumination, rotation degree.&lt;/p&gt;
&lt;h2&gt;Questions and promising applications&lt;/h2&gt;
&lt;p&gt;Object Recognition by keypoints matching.&lt;/p&gt;
&lt;h2&gt;Technical Summarization&lt;/h2&gt;
&lt;p&gt;Scale Invariant Feature Transform (SIFT) contains two main part: finding keypoints and describing them. The process contains four steps:&lt;/p&gt;
&lt;h3&gt;Step 1&lt;/h3&gt;
&lt;p&gt;Scale-space extrema detection To find keypoints, the first thing is to detect scale-space extrema points, where are significant different from their neighbors. Koenderink (1984) and Lindeberg (1994) showed that the only possible scale-space kernel is the Gaussian function. Using the Gaussian function convolution on the image with varying scales, calculate the difference from the nearby two scales separated ny a constant factor k for finding the points whose difference is local maximum or minimum.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img1" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.32.36.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="img2" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.34.58.png" /&gt;&lt;/p&gt;
&lt;p&gt;Two reasons support this method: (a) it is a particularly efficient function to compute (b) In addition, the difference-of-Gaussian function provides a close approximation to the scale-normalized Laplacian of Gaussian, σ2∇2G, as studied by Lindeberg (1994)&lt;/p&gt;
&lt;h3&gt;Step 2. Keypoint localization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img3" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.40.52.png" /&gt;&lt;/p&gt;
&lt;p&gt;In order to detect the local maxima and minima of D(x, y, σ), each sample point is compared to its eight neighbors in the current image and nine neighbors in the scale above and below (see Figure 2)Three problem left: a. k=? b. σ=? c. How many octaves?After finding keypoints, the next problem is to accurate the keypoint localization and eliminate edge responses.&lt;/p&gt;
&lt;h3&gt;Step 3 Orientation assignment&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img4" src="/images/ammai/03/螢幕快照-2012-03-13-下午10.43.34.png" /&gt;&lt;/p&gt;
&lt;p&gt;Calculate the magnitude and the orientation of the gradient of keypoints. To make the descriptor is invariant to rotation, the orientation can be represented relative to the the dominant direction.&lt;/li&gt;&lt;/p&gt;
&lt;h3&gt;Step 4 Keypoint descriptor&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img5" src="/images/ammai/03/螢幕快照-2012-03-13-下午11.11.08.png" /&gt;&lt;/p&gt;
&lt;p&gt;To describe the sample point, use m x m pixels within n x n blocks surrounding the point with k orientation bins. Sum up the orientations of m x m pixels to k bins weighted by magnitudes for each block. Therefore, one sample point would have n * n * k dimensions features vector to represent.The vector is normalized to unit length for the purpose that let the descriptor is invariant to the effects of illumination change, such as contrast, causing pixels value  multiplied with a constant factor.Besides, a non-linear illumination changes may be caused by 3D surfaces with differing orientations or  camera saturation. To cope with it, reduce the inﬂuence of large gradient magnitudes by thresholding the values in the unit feature vector to each be no larger than 0.2, and then renormalizing to unit length&lt;/p&gt;</summary></entry><entry><title>"PLSI &amp; LDA"</title><link href="http://leohung.net/plsi-lda.html" rel="alternate"></link><updated>2013-09-07T23:56:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:plsi-lda.html</id><summary type="html">&lt;p&gt;PLSI &amp;amp; LDA&lt;/p&gt;
&lt;p&gt;Origin source: "Probabilistic latent semantic indexing," T. Hofmann, SIGIR, 1999. "Latent Dirichlet allocation," D. Blei, A. Ng, and M. Jordan. . Journal of Machine Learning Research, 3:993–1022, January 2003 The following figures and formula are copied from above papers.&lt;/p&gt;
&lt;p&gt;PLSI and LDA are both hidden-class decomposition method ( or dimension reduction method ) but are different in modeling approaches; through mapping documents from high-dimension to low-dimension hidden class space, the ambiguous words relationship can be detected such that improve information retrieval precision.&lt;/p&gt;
&lt;h2&gt;PLSI&lt;/h2&gt;
&lt;p&gt;PLSI use the hidden-class decomposition technique PLSA, which is a generative probability model assuming a word in a specific document is generated as follows: (1) randomly pick a hidden-topic z for the document following P(z|d) (2) randomly pick a word w conditioning the topic z following P(w|z) (3) sum up all hidden topic result of generating word w. In other words, wordis relative to unobservable hidden-topic whose probability can not be calculated directly except an EM method, which compose with two steps: expectation step and maximization step.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pdm" src="/images/ammai/04/Pdm.png" /&gt;&lt;/p&gt;
&lt;p&gt;In E step, calculate posterior probabilities given document and word as following formula.&lt;/p&gt;
&lt;p&gt;&lt;img alt="EStep" src="/images/ammai/04/EStep.png" /&gt;&lt;/p&gt;
&lt;p&gt;In M step, calculate prior probabilities.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MStep" src="/images/ammai/04/MStep.png" /&gt;&lt;/p&gt;
&lt;p&gt;Besides, to avoid overfitting, the authors proposed TEM method, which adds a power coefficient $latex \beta $ to original EM formula as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="TEM" src="/images/ammai/04/TEM.png" /&gt;&lt;/p&gt;
&lt;h2&gt;LDA&lt;/h2&gt;
&lt;p&gt;LDA is a graphical model approach to model words generation process.&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA" src="/images/ammai/04/LDA.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;p&gt;For PLSI, the formula in the M-step minimize word perplexity directly. For LDA, because LDA is a graphical model, LDA can be extended to more complex model.&lt;/p&gt;</summary></entry><entry><title>"Nonlinear dimensionality reduction by locally linear embedding"</title><link href="http://leohung.net/nonlinear-dimensionality-reduction-by-locally-linear-embedding.html" rel="alternate"></link><updated>2013-09-07T23:49:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:nonlinear-dimensionality-reduction-by-locally-linear-embedding.html</id><summary type="html">&lt;p&gt;"Nonlinear dimensionality reduction by locally linear embedding," Roweis &amp;amp; Saul, Science, 2000.&lt;/p&gt;
&lt;p&gt;The following figures are all copied from the essay.&lt;/p&gt;
&lt;p&gt;&lt;img alt="nonlinear-dimension-reduction" src="/images/ammai/04/nonlinear-dimension-reduction.png" /&gt;&lt;/p&gt;
&lt;p&gt;The main purpose of this method is to reduce the data from high dimensions to low dimensions while maintaining instances local relative distances at the same time. There are three main steps: (1) selecting k neighbors (2) calculate contribution weights $latex W_{ij} $ (3) reducing dimensions. The algorithm can be used for not only text corpus, but also images.&lt;/p&gt;
&lt;p&gt;First, select the k nearest neighbors for each instance, where "k" is a parameter,  and distances between between neighbors will be maintain.&lt;/p&gt;
&lt;p&gt;Second, calculate the weights  $latex W_{ij} $,  the contribution of $latex j$ to $latex i$ , between the selected instance $latex i $ and its neighbors $latex j $ by minimizing the following cost function with two constraints: (1) $latex X_j $ are the neighbors of $latex X_i $. (2)&lt;/p&gt;
&lt;p&gt;&lt;img alt="constraint2" src="/images/ammai/04/constraint2.gif" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="cost-function-1" src="/images/ammai/04/cost-function-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Third, with the following cost function fixing $latex W_{ij}$ , transform instances from high-dimension X to low-dimension Y, which can be seen as a eigenvalue problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cost-function-2" src="/images/ammai/04/cost-function-2.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;p&gt;(1) Invariant Due to the cost function is a sum function, for image processing, it will be not sensitive to geometric transformation.
(2) Small amount of parameters The method use small amount of parameters, only k for selecting neighbors. ##&lt;/p&gt;
&lt;h2&gt;Disadvantages&lt;/h2&gt;
&lt;p&gt;If selecting neighbors with brute force comparison, it will consume much of time when data size raising. May be it can be solved by locality sensitive hash method to find nearest neighbors efficiently.&lt;/p&gt;</summary></entry><entry><title>"Support vector learning for ordinal regression"</title><link href="http://leohung.net/support-vector-learning-for-ordinal-regression.html" rel="alternate"></link><updated>2013-09-07T23:43:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:support-vector-learning-for-ordinal-regression.html</id><summary type="html">&lt;p&gt;"Support vector learning for ordinal regression," R. Herbrich, T. Graepel, K. Obermayer, ICANN, 1999&lt;/p&gt;
&lt;p&gt;Authors: R. Herbrich, T. Graepel, K. Obermayer
The following figures are all copies from the paper.&lt;/p&gt;
&lt;p&gt;The classical supervised machine learning methods deal with classification problem and regression problem. Instead, this research proposed a method to deal with ordinal regression, predicting the order of samples, by reducing ordinal regression problem into a classification problem on pair of object.&lt;/p&gt;
&lt;p&gt;Assume input space X ={x1, x2,...xk }, and output space  Y = {y1, y2 ...yk} where exist a order that y1 &amp;lt; y2 &amp;lt; y3 ... &amp;lt; yk. The ordinal regression problem is to find a model h mapping from X to Y such that minimize the loss function, which panelize wrong order mappings.&lt;/p&gt;
&lt;p&gt;To transfer it into classification problem, define a function p = p( h(x_1), h(x_2) ) ={-1, 0 ,+1} which means the classes of order: &amp;lt; , = , or &amp;gt;. Then, the classification problem can be transformed to find a model h that mapping from (h(x_1), h(x_2) ) to ( y_1, y_2) to minimize the loss function which is related to whether p(h(x_1),h(x_2)) is as same as p(y_1,y_2).&lt;/p&gt;
&lt;p&gt;After transformation, it can be solved by support vector machine. The experiment compared ordinal regression with multi-class SVM and support vector regression(SVR). The result showed that ordinal regression is better than others.&lt;/p&gt;
&lt;p&gt;&lt;img alt="r_measure" src="/images/ammai/05/r_measure.png" /&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, the research experimented on a IR problem, and it shows that ordinal regression performed better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IR_result" src="/images/ammai/05/IR_result.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My Opinion&lt;/h2&gt;
&lt;p&gt;According to the authors said, these work introduced a new problem in machine learning filed, and they proposed a novel and simple method to deal with ordinal regression problem. However, due to reducing process using combination of instance, I think that it will become become a big problem when the dataset grossing. When the number of instance gross  to n, the number of combination gross to O(n^2), which is hard to compute if n is large.&lt;/p&gt;</summary></entry><entry><title>"Learning to rank: from pairwise approach to listwise approach"</title><link href="http://leohung.net/learning-to-rank-from-pairwise-approach-to-listwise-approach.html" rel="alternate"></link><updated>2013-09-07T23:25:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:learning-to-rank-from-pairwise-approach-to-listwise-approach.html</id><summary type="html">&lt;p&gt;"Learning to rank: from pairwise approach to listwise approach," Cao, ICML, 2007.&lt;/p&gt;
&lt;p&gt;Authors: Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, Hang Li The following figures are all copied from this paper.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;Unlike pairwise rank-learning method like RankSVM, this work proposed a listwise rank learning framework where training instances are rank lists instead of document pairs. To predict a rank list for a query $latex q_i $, this work models the score of the rank permutation as a probability function as follows, in which $latex s_{\pi(j)} $ is a scoring function for a document in j position and \phi is a positive strict increasing function often using exponential function.&lt;/p&gt;
&lt;p&gt;&lt;img alt="prob_func" src="/images/ammai/06/prob_func.png" /&gt;&lt;/p&gt;
&lt;p&gt;The objective function of learning is to minimize the loss of list between training data and predicting data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="objective_function" src="/images/ammai/06/objective_function.png" /&gt;&lt;/p&gt;
&lt;p&gt;To reduce computation complexity, it use top K probability modeling the likelihood of top K documents order. In this research, k is selected as 1, and the scoring function is trained by Neural network optimized by gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;img alt="top-k-prob" src="/images/ammai/06/top-k-probability.png" /&gt;&lt;/p&gt;
&lt;p&gt;And the loss function will be defined as the cross entropy between training data and predicting list.&lt;/p&gt;
&lt;p&gt;&lt;img alt="top_k_loss_func" src="/images/ammai/06/top_k_loss_func.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Experiments Result&lt;/h2&gt;
&lt;p&gt;Comparing with pairwise methods: RankBoost, RankSVM and RankNet, ListNet have  better results in both MAP and NDCP measurements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="map" src="/images/ammai/06/map.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="NDCG" src="/images/ammai/06/NDCG.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My Opinion&lt;/h2&gt;
&lt;p&gt;It is charming that ListNet directly model the retrieval directly based on ranking list, which is still the search engine paradigm today. However, I am curious about why the probability of permutations performs well and results with different parameters "K": beyond the consideration of computation complexity, will it work more precisely with a larget "K"?&lt;/p&gt;</summary></entry><entry><title>"A Global Geometric Framework for Nonlinear Dimensionality Reduction"</title><link href="http://leohung.net/a-global-geometric-framework-for-nonlinear-dimensionality-reduction.html" rel="alternate"></link><updated>2013-09-07T23:09:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:a-global-geometric-framework-for-nonlinear-dimensionality-reduction.html</id><summary type="html">&lt;p&gt;Authors: Joshua B. Tenenbaum, Vin de Silva, John C. Langford&lt;/p&gt;
&lt;p&gt;The following figures are copied from the original paper.&lt;/p&gt;
&lt;p&gt;This work introduces a method called Isomap for dimension reduction specifically on non-linear data. The algorithm composes of three steps:&lt;/p&gt;
&lt;p&gt;(1) Find neighbors for each points in input space. To do so, it can be done (a) by retaining K neighbors or (b) by retaining neighbors in a constat radius range for each point. Then, represents neighbors relationship as a weighted graph, in which the weight is the distance between two points.&lt;/p&gt;
&lt;p&gt;(2) Compute shortest paths length for all pair points on the neighbors graph. Using the all pair shortest path algorithm like Floyd’s algorithm to detect all pair shortest path lengths.&lt;/p&gt;
&lt;p&gt;(3) Perform dimension reduction by applying MDS. Transfer data into low d-dimensions space Y by performing MDS on pair distances calculated in step (2). Like MDS and PCA, the true dimension d in Isomap can be estimated from error loss decreasing in transfered space Y when d increases, and also Isomap can recover true data structure if there are sufficient data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="result" src="/images/ammai/06/result-300x229.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;My Opinon&lt;/em&gt;: As the experiment result showed above, Isomap finds the structure of data well. However, just authors mentioned, it takes much data to detect the structure of a specific object, and also the time complexity is high due to the step 2 O(n^3) algorithm. I am curious that whether it is useful or not in the real application context.&lt;/p&gt;</summary></entry></feed>