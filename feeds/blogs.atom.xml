<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LeoHung.self()</title><link href="http://leohung.net/" rel="alternate"></link><link href="http://leohung.net/feeds/blogs.atom.xml" rel="self"></link><id>http://leohung.net/</id><updated>2015-12-14T01:30:00+01:00</updated><entry><title>Apache Spark 環境變數調整</title><link href="http://leohung.net/apache-spark-huan-jing-bian-shu-diao-zheng.html" rel="alternate"></link><updated>2015-12-14T01:30:00+01:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2015-12-14:apache-spark-huan-jing-bian-shu-diao-zheng.html</id><summary type="html">&lt;p&gt;目前使用 Apache Spark 的心得是，Spark真的很棒，處理計算時間快得沒話說。但是他許多預設的參數，值得再做更多進一步的調整。這篇整理了我這一學期做實驗的操作心得：哪些環境變數該調，又該怎麼調。如果你有不同的見解或其他推薦的參數和調整方向，還請忘不吝惜指教批評。&lt;/p&gt;
&lt;h2&gt;怎麼改&lt;/h2&gt;
&lt;p&gt;可以透過修改 $spark_dir/conf/spark-defaults.conf 達成。&lt;/p&gt;
&lt;h2&gt;spark.serializer&lt;/h2&gt;
&lt;p&gt;Spark 預設使用 Java 的 ObjectOutputStream framework 處理物件的序列化(serialize)。不過官網建議使用另一套 KryoSerializer，效能處理的速度會比較快。&lt;/p&gt;
&lt;p&gt;spark.serializer                 org.apache.spark.serializer.KryoSerializer&lt;/p&gt;
&lt;h2&gt;spark.driver.memory&lt;/h2&gt;
&lt;p&gt;Spark 的 driver 所先佔有的記憶體其實很少，如果有用到 collectAsMap ，driver會把分散在各個 worker 的資料蒐集回 driver 形成物件。如果 driver 記憶體不夠大，collectAsMap又形成過大的物件，會導致記憶體不足 (out of memory) 的錯誤，造成整個工作失敗。&lt;/p&gt;
&lt;p&gt;e.g.
  spark.driver.memory              55047m&lt;/p&gt;
&lt;h2&gt;spark.locality.wait&lt;/h2&gt;
&lt;p&gt;Spark 的 worker 處理任務的時候，會優先處理同一台機器上的任務。如果有 worker 忙不過來，且又有 worker 又處於空閑狀態，他就會搜羅別台 worker 的任務來做，來幫其他worker做外包。&lt;/p&gt;
&lt;p&gt;但是有些任務不適合外包，因為他原本所預定處理的數據量太大，一外包任務就還得傳輸這些數據，反而增加處理時間。還不如等一下，讓那個原本忙不過來的 worker 來處理自身的任務還比較快。&lt;/p&gt;
&lt;p&gt;spark.locality.wait 這個參數定義要等多久，才會把一個任務外包給其他的worker。如果設定很大的數值，就幾乎強迫是由同一台 worker 處理他自己的任務。&lt;/p&gt;
&lt;p&gt;e.g.
  spark.locality.wait              3000000&lt;/p&gt;</summary><category term="spark"></category><category term="parameter"></category></entry><entry><title>Easy to read ML papers and blogs</title><link href="http://leohung.net/easy-to-read-ml-papers-and-blogs.html" rel="alternate"></link><updated>2015-11-24T01:30:00+01:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2015-11-24:easy-to-read-ml-papers-and-blogs.html</id><summary type="html">&lt;h1&gt;EM&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;从最大似然到EM算法浅解. &lt;a href="http://blog.csdn.net/zouxy09/article/details/8537620"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;（EM算法）The EM Algorithm. &lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;PCA &amp;amp; SVD&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Jonathon Shlens (2014) A Tutorial on Principal Component Analysis. &lt;a href="http://arxiv.org/pdf/1404.1100.pdf"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="machine learning"></category><category term=""></category></entry><entry><title>分布式巨量機械學習系統模型: MapReduce, Data Graph, Parameter Server</title><link href="http://leohung.net/fen-bu-shi-ju-liang-ji-jie-xue-xi-xi-tong-mo-xing-mapreduce-data-graph-parameter-server.html" rel="alternate"></link><updated>2015-05-13T01:30:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2015-05-13:fen-bu-shi-ju-liang-ji-jie-xue-xi-xi-tong-mo-xing-mapreduce-data-graph-parameter-server.html</id><summary type="html">&lt;h2&gt;總結&lt;/h2&gt;
&lt;p&gt;巨量機械學習需要處理的挑戰有二：(1) 訓練資料集過大 (2) 機械學習模型本身過於複雜，所需要訓練的參數過多。這兩個原因使得無法使用單一主機解決巨量機械學習的問題。為此，許多團隊於近幾年提出分布式機械學習系統，好著手處理巨量資料的挑戰。本篇整理了目前分布式巨量機械學習系統的三種典範：MapReduce, Data Graph, 和 Parameter Server.&lt;/p&gt;
&lt;h2&gt;為什麼我們需要分布式系統處理巨量機械學習？&lt;/h2&gt;
&lt;p&gt;"巨量"有兩種意義：(1) 訓練資料集過大 (2) 模型本身過度複雜，所需訓練的參數過多。因為這兩個原因，通常無法使用單一機器處理巨量機械學習的問題。為此，開發者得需要設計分布式系統，或者建立在已有的分布式計算框架來處理。&lt;/p&gt;
&lt;p&gt;根據目前主流巨量機械學習系統的架構設計，可歸納為三種模式： MapReduce, Data Graph, 和 Parameter Server。&lt;/p&gt;
&lt;h2&gt;MapReduce 模式&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="mapreduce" src="/images/bigml/mapreduce.png" /&gt;&lt;/p&gt;
&lt;h4&gt;圖片 1. MapReduce  (資料來源: [9])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;MapReduce [9] 是很熱門的分布式計算框架, 而 Apache Hadoop [10] 則是對應的開源系統. 一般來說，MapReduce 有兩個階段：Map 和 Reduce。在 Map 階段，系統根據使用者提供的 Map 函式將輸入資料轉換成 key-value 的輸出資料。之後，MapReduce 系統會將同樣 key 的資料集合放到同一台機器做 Reduce 階段。 Reducer 會收到 key，和對應一連串的 value，根據使用者的函式做對應的操作，在儲存回分布式的資料系統裡面。&lt;/p&gt;
&lt;p&gt;由於 MapReduce 是一套相當成熟的框架，許多研究致力於將原本的機械學習演算法拆解成多個循環組成的 Map 和 Reduce，將演算法實作在 MapReduce 框架上。其中 Mahout [1] 是一個很知名的開源專案，就是將機械學習演算法建立在 Hadoop之上 (注: Mahout社群在2014年4月決定將算法轉向開發在 Spark 上)。&lt;/p&gt;
&lt;p&gt;Hadoop預設的設計上會把 Mapper 中間產生的 key-value 組，以及 Reducer 產生的key-value 組存回硬碟。但是，許多機械學習演算法需要多次的循環計算，直到符合終止條件 (例如: gradient decent)。多次的循環計算產生巨量的硬碟存儲，拖慢了整體計算的效率。&lt;/p&gt;
&lt;p&gt;為了解決這個問題， Spark [2] 提供 Resilient Distributed Datasets (RDD)，使用者可以指定將中介的計算結果存在記憶體當中（如果計算結果太大，則會存進硬碟），節省硬碟存儲的時間。根據他們的實驗，用 Spark 實作的 logistic regrsssion 和 k-means 演算法，可以比用 Hadoop 實作的版本快上20倍。 其中，MLlib 就是建立在 Spark 上熱門的巨量機械學習開源專案。&lt;/p&gt;
&lt;p&gt;另外， Low 等人 (2012) [3] 指出建立在 MapReduce 上的機械學習演算法，通常會在 Map 階段產生重複內容的訊息傳遞給 Reduce，增加網路傳輸的成本。以 Pagerank為例，高 degree 的節點需要將自己的 Pagerank 值複製多份，以傳給所有的鄰居節點，而增加 Shuffle 階段的成本。&lt;/p&gt;
&lt;h2&gt;Data Graph 模式&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="datagraph" src="/images/bigml/graphlab.png" /&gt;&lt;/p&gt;
&lt;h4&gt;圖片 2. Data Graph (資料來源: [3])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Data graph 模式將參數之間的計算依賴關係建構成 data graph: 節點是參數，邊則是節點跟節點之間的計算依賴關係。&lt;/p&gt;
&lt;p&gt;以 Pagerank 為例。在 data graph當中，每一個節點儲存對應的原圖節點的 Pagerank值，如果原圖兩節點中有連結，則在 data graph 裡兩個節點也會有連結。 data graph 當中的節點，僅可以操作到鄰居節點以及連結邊上的數值。一個節點的工作則是: 將自身的 Pagerank 值，散布給所有的鄰居。&lt;/p&gt;
&lt;p&gt;Pregel [4] 是由 Google 提出的 data graph 計算框架。 Pregel 會以連續的超階段 (supersteps) 的方式執行使用者程式。在第 S 個超階段，框架會平行地執行使用者所提供的節點函式，節點可以收到第 S - 1 超階段傳遞給他的值，加以計算，在傳遞給鄰居節點，其鄰居節點會在 S + 1 超階段接收到值。等到所有節點都完成計算，才能一起進入到 S + 1 超階段。Pregel 很適合處理圖形演算法，像是 Pagerank, 最短路徑, 或者圖形分群。雖然 Google 並沒有將 Pregel 開源化，但目前在開源社群以有對應的開源版本，其名稱是 Apache Giraph。&lt;/p&gt;
&lt;p&gt;GraphLab [3] 是由 CMU 實驗室團隊所提出的分布式 data graph 計算框架。不同於 Pregel，GraphLab 並不強制使用者的程序必須要等到所有的節點都結束計算，才能到下一個超階段。而是，它允許節點能夠異步地 (asynchronously) 執行程式。此外，GraphLab 提供三種不同強度的一致性模型 (consistency model)：vertex consistency, edge consistency, 和 full consistency。用以確保當一個節點在執行程式的時候，不會有另外一個程序也在更改同樣的鄰居節點，而產生的一致性問題。&lt;/p&gt;
&lt;p&gt;PowerGraph [5] 則是 GraphLab 強化版本，設計處理圖形中普遍存在的超高 Degree 節點會產生的問題。此外，有另一個版本 GraphChi [6] ，則是致力於只用一台機器，實作出 GraphLab 的計算邏輯。&lt;/p&gt;
&lt;h2&gt;Parameter Server 模式&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="parameter_server" src="/images/bigml/parameter_server.png" /&gt;&lt;/p&gt;
&lt;h4&gt;圖形 3. Parameter Server (資料來源: [7])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Parameter Server [7] 由 server 集群和 worker 集群所組成。訓練資料集合是散步在 worker 當中， worker 根據自己所擁有的訓練資料計算模型參數。而 Server 維護分步式的 Hashtable，使得 worker 可以從中擷取正在計算中的模型參數，以及設置其計算的結果。&lt;/p&gt;
&lt;p&gt;以 logistic regression 為例子，一個計算的循環如下: worker 先從 server 取得目前最新的模型參數。接著，worker 根據本地的資料開始計算 gradient descent value ，算完之後，再把 gradient descent 的值回傳給 server 做加總。&lt;/p&gt;
&lt;p&gt;Petuum [8] 也是一套基於 Parameter Server 設計的計算框架。作者們提出 Stale Synchronous Parallel (SSP) 的通訊概念，以減少為了同步問題，worker 的等待時間。使用者可以設置 Stale 的值 S。當一個跑得比較快的 worker 已經算好一個模型參數值，他可以不用等其他也會算到相同參數的 worker，繼續在本地更新模型的參數，直到比較快的 worker 跟比較慢的 worker 所計算的模型參數版本差了 S 個版本，比較快的 worker 才需要停下來等待。&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Mahout. http://mahout.apache.org. (n.d.). Mahout. http://mahout.apache.org. Retrieved February 22, 2015, from http://mahout.apache.org/&lt;/p&gt;
&lt;p&gt;[2] Zaharia, M., Chowdhury, M., Das, T., Dave, A., Ma, J., McCauly, M., et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. Nsdi, 15–28.&lt;/p&gt;
&lt;p&gt;[3] Low, Y., Gonzalez, J. E., Kyrola, A., Bickson, D., Guestrin, C., &amp;amp; Hellerstein, J. M. (2014). GraphLab: A New Framework For Parallel Machine Learning. CoRR Abs/1204.6078, cs.LG.&lt;/p&gt;
&lt;p&gt;[4] Malewicz, G., Austern, M. H., Bik, A. J. C., Dehnert, J. C., Horn, I., Leiser, N., &amp;amp; Czajkowski, G. (2010). Pregel: a system for large-scale graph processing. Sigmod, 135–146. http://doi.org/10.1145/1807167.1807184&lt;/p&gt;
&lt;p&gt;[5] Gonzalez, J. E., Low, Y., Gu, H., Bickson, D., &amp;amp; Guestrin, C. (2012). PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. Osdi, 17–30.&lt;/p&gt;
&lt;p&gt;[6] Kyrola, A., Blelloch, G. E., &amp;amp; Guestrin, C. (2012). GraphChi: Large-Scale Graph Computation on Just a PC. Osdi, 31–46.&lt;/p&gt;
&lt;p&gt;[7] Li, M., Andersen, D. G., Smola, A. J., &amp;amp; Yu, K. (2014). Communication Efficient Distributed Machine Learning with the Parameter Server. Nips, 19–27.&lt;/p&gt;
&lt;p&gt;[8] Dai, W., Wei, J., Zheng, X., Kim, J. K., Lee, S., Yin, J., et al. (2013). Petuum: A Framework for Iterative-Convergent Distributed ML. CoRR Abs/1204.6078, stat.ML.&lt;/p&gt;
&lt;p&gt;[9] Dean, J., &amp;amp; Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1). http://doi.org/10.1145/1327452.1327492&lt;/p&gt;
&lt;p&gt;[10] Hadoop. http://hadoop.apache.org. (n.d.). Hadoop. http://hadoop.apache.org. Retrieved February 22, 2015, from http://hadoop.apache.org/&lt;/p&gt;
&lt;p&gt;[11] Giraph. http://giraph.apache.org. (n.d.). Giraph. http://giraph.apache.org. Retrieved February 22, 2015, from http://giraph.apache.org/&lt;/p&gt;</summary><category term="機械學習"></category><category term="分布式系統"></category><category term="巨量機械學習"></category></entry><entry><title>Large-scale Distributed Machine Learning: MapReduce, Data Graph, and Parameter Server</title><link href="http://leohung.net/large-scale-distributed-machine-learning-mapreduce-data-graph-and-parameter-server.html" rel="alternate"></link><updated>2015-05-13T01:30:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2015-05-13:large-scale-distributed-machine-learning-mapreduce-data-graph-and-parameter-server.html</id><summary type="html">&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Big machine learning is hard because (1) the amount of traning data is huge (2) the number of parameters is large. This note summarizes three paradigms of distributed architecture for large-scale machine learning problem: MapReduce, Data Graph, and Parameter Server.&lt;/p&gt;
&lt;h2&gt;Why should we build distributed system for large-scale machine Learning?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Large-scale&lt;/em&gt; has two meanings: (1) the amount of training data is huge which can not easily fit in a machine. (2) the number of parameters is too large to be stored and manipulated by a single machine. Therefore, the large-scale machine learning and data mining usually cannot be solved by a single machine. In stead, developers need to construct their machine learning algorithms into a distributed system.&lt;/p&gt;
&lt;h2&gt;MapReduce Approach&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="mapreduce" src="/images/bigml/mapreduce.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Figure 1. MapReduce  (source: [9])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;MapReduce [9] is a well-known distributed computing model on clusters, and Apache Hadoop [10] is the best known open source counterpart for MapReduce. In general, MapReduce contains two phases: map phase and reduce phase. In map phase, the provided map program should process the input chunks into key-value pairs. After map phase, the key-value pairs will be shuffled into the specific reduce nodes according to the hash value of the key. The reducers will receive the list of values with the corresponding key, perform its reduce function, and store the output into a distributed file system.&lt;/p&gt;
&lt;p&gt;Many research tried to fit the existing machine learning algorithms into MapReduce framework. They need to transform original algorithms into map functions and reduce functions in multiple iterations. Mahout [1] is a well-known open source for large-scale machine learning mainly based on Hadoop.&lt;/p&gt;
&lt;p&gt;Hadoop is designed to store the output of one map-reduce phase into disk; however, many machine learning algorithms need iterative calculation until the termination condition is achieved (e.g. the algorithms in gradient decent fashion). Multiple iteration computing on Hadoop can cause huge amount of I/O time and deteriorate the performance.&lt;/p&gt;
&lt;p&gt;To tackle with this problem, Spark [2] provides Resilient Distributed Datasets (RDD) for in-memory computation, which can cache the output of one MapReduce iteration into memory and load it from memory in the next iteration, so RDD can improve performance by avoiding disk I/O. According to their experiments, Spark can have 20x speedup over Hadoop in logistic regression and k-means algorithm. MLlib is a popular machine learning framework based on Spark.&lt;/p&gt;
&lt;p&gt;Another problem for MapReduce approach mentioned by Low et al. (2012) [3] is that the framework may generate huge amount of duplicated messages in the map phase and increase network cost. Taking Pagerank as an example, the high degree nodes need to send multiple duplication of its Pagerank value to its neighbors in map phase. The network traffic may reduce the performance of the algorithms.&lt;/p&gt;
&lt;h2&gt;Data Graph Approach&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="datagraph" src="/images/bigml/graphlab.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Figure 2. Data Graph (source: [3])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The data graph approach means to model the computation relationship between the variables in the machine learning algorithm into a data graph: the nodes are the variables and the edges are the connection of message passing.&lt;/p&gt;
&lt;p&gt;Take Pagerank for example (which is usually the "Hello World" program for Data Graph approach). Each node in data graph represents stores the Pagerank value for a vertex in original graph. The links in data graph represent the connections between vertices. A node in a data graph can access the values on it's neighbor nodes and adjcent links. The Pagerank job of a node in the data graph is to distribute its Pagerank value to the neigbor nodes.&lt;/p&gt;
&lt;p&gt;Pregel [4] is a distributed graph computing framework. The programs in Pregel are executed in continuous supersteps. In each superstep S, the framework call the defined functions on the vertices in parallel, and the vertex can read the message sent to it in S-1, and also send messages to other vertices which can read the message in S+1 superstep. Pregel is well suited in graph algorithms like Pagerank, shortest path, and clustering. Although Google does not release the source code of Pregel to public, but it has an open source counterpart called Apache Giraph [11].&lt;/p&gt;
&lt;p&gt;GraphLab [3] is an another distributed graph computing framework. Unlike Pregel, GraphLab does not have synchronous supersteps for message passing. Instead, it uses vertex functions in asynchronous fashion to specify what kind of computation is performed and which vertices will run in the next round, and it uses three different consistency models (vertex consistency, edge consistency, and full consistency) to assure serializablility. GraphLab is improved in the next version PowerGraph [5], which can deal with high degree vertices by splitting them into multiple vertices. Besides, GraphLab has an variant called GraphChi [6], which focuses on how to implement graph computation logic in a single machine.&lt;/p&gt;
&lt;h2&gt;Parameter Server Approach&lt;/h2&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="parameter_server" src="/images/bigml/parameter_server.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Figure 3. Parameter Server (source: [7])&lt;/h4&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Parameter Server [7] is a servers-workers style distributed computing framework. The servers storing global variables in distributed hashtable fashion, supporting pull, push, and aggregation operations of global variables for the workers. The training data is partitioned in the workers where the main computation happens.&lt;/p&gt;
&lt;p&gt;Taking distributed logistic regression as an example, one calculation cycle is as follows: the workers pull the global weights from the servers. Each worker computes its local gradient descent value according its data, and then push back to the servers for aggregation. After the aggregation, the workers pull the global weights again.&lt;/p&gt;
&lt;p&gt;Petuum [8] also follows Parameter Server architecture. They developed a more specific communication mechanism called Stale Synchronous Parallel(SSP) to mitigate the workers from waiting. Users can set the parameter staleness S. A fastest worker need to wait if it is faster than the slowest worker S iterations. SSP can reduce the network traffic because workers do not need to spend too much time waiting for other worker updates.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Mahout. http://mahout.apache.org. (n.d.). Mahout. http://mahout.apache.org. Retrieved February 22, 2015, from http://mahout.apache.org/&lt;/p&gt;
&lt;p&gt;[2] Zaharia, M., Chowdhury, M., Das, T., Dave, A., Ma, J., McCauly, M., et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. Nsdi, 15–28.&lt;/p&gt;
&lt;p&gt;[3] Low, Y., Gonzalez, J. E., Kyrola, A., Bickson, D., Guestrin, C., &amp;amp; Hellerstein, J. M. (2014). GraphLab: A New Framework For Parallel Machine Learning. CoRR Abs/1204.6078, cs.LG.&lt;/p&gt;
&lt;p&gt;[4] Malewicz, G., Austern, M. H., Bik, A. J. C., Dehnert, J. C., Horn, I., Leiser, N., &amp;amp; Czajkowski, G. (2010). Pregel: a system for large-scale graph processing. Sigmod, 135–146. http://doi.org/10.1145/1807167.1807184&lt;/p&gt;
&lt;p&gt;[5] Gonzalez, J. E., Low, Y., Gu, H., Bickson, D., &amp;amp; Guestrin, C. (2012). PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. Osdi, 17–30.&lt;/p&gt;
&lt;p&gt;[6] Kyrola, A., Blelloch, G. E., &amp;amp; Guestrin, C. (2012). GraphChi: Large-Scale Graph Computation on Just a PC. Osdi, 31–46.&lt;/p&gt;
&lt;p&gt;[7] Li, M., Andersen, D. G., Smola, A. J., &amp;amp; Yu, K. (2014). Communication Efficient Distributed Machine Learning with the Parameter Server. Nips, 19–27.&lt;/p&gt;
&lt;p&gt;[8] Dai, W., Wei, J., Zheng, X., Kim, J. K., Lee, S., Yin, J., et al. (2013). Petuum: A Framework for Iterative-Convergent Distributed ML. CoRR Abs/1204.6078, stat.ML.&lt;/p&gt;
&lt;p&gt;[9] Dean, J., &amp;amp; Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1). http://doi.org/10.1145/1327452.1327492&lt;/p&gt;
&lt;p&gt;[10] Hadoop. http://hadoop.apache.org. (n.d.). Hadoop. http://hadoop.apache.org. Retrieved February 22, 2015, from http://hadoop.apache.org/&lt;/p&gt;
&lt;p&gt;[11] Giraph. http://giraph.apache.org. (n.d.). Giraph. http://giraph.apache.org. Retrieved February 22, 2015, from http://giraph.apache.org/&lt;/p&gt;</summary><category term="machine learning"></category><category term="distributed system"></category><category term="big-ml"></category></entry><entry><title>"NetworkX Network: Discovering the core of company network in Taiwan by using NetworkX. (用NetworkX找出台灣公司網絡核心) @ PyCon TW 2014"</title><link href="http://leohung.net/networkx-network-discovering-the-core-of-company-network-in-taiwan-by-using-networkx-yong-networkxzhao-chu-tai-wan-gong-si-wang-luo-he-xin-pycon-tw-2014.html" rel="alternate"></link><updated>2014-05-17T01:30:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2014-05-17:networkx-network-discovering-the-core-of-company-network-in-taiwan-by-using-networkx-yong-networkxzhao-chu-tai-wan-gong-si-wang-luo-he-xin-pycon-tw-2014.html</id><summary type="html">&lt;h1&gt;Slide&lt;/h1&gt;
&lt;p&gt;&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/34775422" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="https://www.slideshare.net/AskusHong/pycon2014" title="NetworkX Network: 用NetworkX找出台灣公司網絡核心 by Leo Hung @PyCon TW 2014" target="_blank"&gt;NetworkX Network: 用NetworkX找出台灣公司網絡核心 by Leo Hung @PyCon TW 2014&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="http://www.slideshare.net/AskusHong" target="_blank"&gt;SanChan Hong&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/LeoHung/pycon2014_tw_company_core_network"&gt;Github Link&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Closeness" src="/images/closeness_50.png" /&gt;&lt;/p&gt;</summary><category term="networkx"></category><category term="python"></category><category term="pycon_2014"></category><category term="pycon_tw"></category></entry><entry><title>"Teaching Elasticsearch reading Chinese in 1 Minutes "</title><link href="http://leohung.net/teaching-elasticsearch-reading-chinese-in-1-minutes.html" rel="alternate"></link><updated>2014-04-26T19:07:00+02:00</updated><author><name>Leo Hung</name></author><id>tag:leohung.net,2014-04-26:teaching-elasticsearch-reading-chinese-in-1-minutes.html</id><summary type="html">&lt;h1&gt;Teaching Elasticsearch reading Chinese in 1 Minutes&lt;/h1&gt;
&lt;p&gt;Elasticsearch(es) is a powerful open source solution for searching, which is serving in wikipedia currently; however, Chinese users is not so lucky with es, becasue the default analyzer of es cannot deal with Chinese well, which will tokenzie Chinese sentences into individual characters, rather than terms, causing large amount of noise in retrieved results.&lt;/p&gt;
&lt;p&gt;To enhance the performance of retrieval, setting the analyzer is necessary; however, there is no quick and effective plug-in for traditional Chinese. IKAnalyzer, developed by China community, may be the best plug-in for Chinese Elasticsearch engineers to serve the job, but IKAnalyzer cannot be set easily.&lt;/p&gt;
&lt;p&gt;If you just want a quick solution, cjk analyzer is the best solution. The algorithm of cjk analyzer is easy to be understood: tokenizing sentences to  bi-gram terms. For example, "我愛吃飯" will be transformed into "我愛", "愛吃", "吃飯". The analyzer is naive but powerful to enhance es performance.&lt;/p&gt;
&lt;p&gt;The below is the setting syntax :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PUT &amp;quot;index name&amp;quot;
&amp;quot;settings&amp;quot;: {
     &amp;quot;analysis&amp;quot;:{
        &amp;quot;analyzer&amp;quot;:{
            &amp;quot;default&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;cjk&amp;quot;}
        }
     }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Beware, the setting only work when the index is empty. If there are data in the index, do not forget to re-index it.&lt;/p&gt;
&lt;h1&gt;一分鐘搞定Elasticsearch中文設定&lt;/h1&gt;
&lt;p&gt;Elasticsearch(es)是一套強大的搜尋引擎套件，目前維基百科正是使用es作為搜尋套件。但是，es的預設設定對中文使用者並不是很友善，因為es預設的分析器(Analyzer)不能處理好中文的斷詞(tokenization)，會把中文詞切成一個字一個字，而非以詞組為單位，使得搜尋效果變差，時常會搜尋到過多雜訊。&lt;/p&gt;
&lt;p&gt;因此，需要設定好es的斷詞設定，才能強化中文搜尋的結果。但可惜的是，繁體中文並沒有已經被最佳化的套件可以無痛與es接軌。目前已知比較好的方案，像是中國社群(誰?)發展出 IKanalyzer ，但需要花一點時間做複雜的設定。&lt;/p&gt;
&lt;p&gt;如果只是短期內需要一個堪用的做法，可以試試看cjk分析器。cjk分析器的原理很簡單，也就只是做 Bi-Gram，把讀進來的句子，兩個兩個切成一個詞。例如：「我愛吃飯」，就變成「我愛, 愛吃, 吃飯」。簡單的設定，就可以強化es的搜尋結果。&lt;/p&gt;
&lt;p&gt;設定的語法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;PUT &amp;quot;index name&amp;quot;
&amp;quot;settings&amp;quot;: {
     &amp;quot;analysis&amp;quot;:{
        &amp;quot;analyzer&amp;quot;:{
            &amp;quot;default&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;cjk&amp;quot;}
        }
     }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;請注意，settings必須在index剛建立之初設定才有效。如果index裡面已經有資料，請reindex你的資料庫。&lt;/p&gt;</summary><category term="Elasticsearch"></category><category term="Chinese"></category><category term="Analyzer"></category></entry><entry><title>"Semi-Supervised Hashing for Scalable Image Retrieval"</title><link href="http://leohung.net/semi-supervised-hashing-for-scalable-image-retrieval.html" rel="alternate"></link><updated>2013-09-08T00:40:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:semi-supervised-hashing-for-scalable-image-retrieval.html</id><summary type="html">&lt;p&gt;J. Wang et al, "Semi-Supervised Hashing for Scalable Image Retrieval," CVPR, 2010.&lt;/p&gt;
&lt;p&gt;The following figures and formula are all copied from this paper.&lt;/p&gt;
&lt;h2&gt;Novelty&lt;/h2&gt;
&lt;p&gt;Unsupervised learning method, like LSH, would ignore the relationship of labels; however, supervised learning method modified from LSH, like RBM or SH, would cost time to compute and require much training data to avoid overfitting. To coping with these problems, the authors introduce a semi-supervised learning method: SSH.&lt;/p&gt;
&lt;h2&gt;Technical Summarization&lt;/h2&gt;
&lt;p&gt;SSH(Semi-Supervised Hashing) is based on two idea: (1) distribute the similar label images to the closer hash (2) make each hash bucket balanced. Assume that $latex M $ is the images pairs set in which have same label, and $latex C $ is the images pairs set where have different labels. The following objective function is main to the first criterion: for pairs with same label, maximize the probability such that they have fall into the closer buckets; for pairs with different, minimize such probability.&lt;/p&gt;
&lt;p&gt;&lt;img alt="03different" src="/images/ammai/03/different.png" /&gt;&lt;/p&gt;
&lt;p&gt;Besides, they add a regularization to constraint such that each bucket size should be balanced: hash functions should be orthogonal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="balance_constraint" src="/images/ammai/03/balance_constraint.png" /&gt;&lt;/p&gt;
&lt;p&gt;Nevertheless, the balancing constraints are hard, so it is replaced by a "soft" constraint, instead of requiring orthogonality, maximizing the variance of mapping space.&lt;/p&gt;
&lt;p style="text-align: center;"&gt;$latex \sum_k{E[ { \left \| h_k(x) - \mu \right \|}^{2} ]}$&lt;/p&gt;

&lt;p&gt;The following figure is the comparison between state-of-the-art methods and SSH in large scale data. They demonstrate that SSH with no-orthogonal performs better than other algorithms&lt;/p&gt;
&lt;p&gt;&lt;img alt="w4_result" src="/images/ammai/03/w4_result.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My comments&lt;/h2&gt;
&lt;p&gt;Just as the authors claim, SSH can maintain the semantic meanings consistency, unlike LSH method, and do not require too much time like supervised learning methods like RBM and SH. Due to SSH is a semi-supervised method, SSH must have label data to train model; however, in reality, the labels are often missing or incomplete. I am curious how to conquer this problem, which is not the main point solved by the study, but I think it is important because it will occur in practice.&lt;/p&gt;</summary></entry><entry><title>"Mairal et al. Online dictionary learning for sparse coding"</title><link href="http://leohung.net/mairal-et-al-online-dictionary-learning-for-sparse-coding.html" rel="alternate"></link><updated>2013-09-08T00:35:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:mairal-et-al-online-dictionary-learning-for-sparse-coding.html</id><summary type="html">&lt;p&gt;Mairal et al. Online dictionary learning for sparse coding. ICML 2009.&lt;/p&gt;
&lt;p&gt;The following figures are all from this paper.&lt;/p&gt;
&lt;p&gt;The paper goal is to propose a online learning method to learn sparse coding dictionary, the basis set which can represent specific data through linear combination, for large scale data.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;&lt;img alt="online_learning_algorithm" src="/images/ammai/03/online_learning_algorithm.png" /&gt;&lt;/p&gt;
&lt;p&gt;The algorithm is showed above. Assuming the training set composed of i.i.d. samples of a distribution, the method is minimizing the cost function, the difference between linear combination of basis set and real data, by stochastic gradient discent to find decomposing coefficient $latex \alpha $ and dictionary $latex D$ iteratively.&lt;/p&gt;
&lt;p&gt;&lt;img alt="online_updating_algorithm" src="/images/ammai/03/online_updating_algorithm.png" /&gt;&lt;/p&gt;
&lt;p&gt;Their algorithm above for updating the dictionary uses block-coordinate descent.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;The work is solid because they present not only the experiment result  but also the sophisticated theoretical proof showing the convergence of the online learning method; however, the assumption, the training set composed of i.i.d. samples of a distribution, may be suspicious because it is not alway satisfied in real world.&lt;/p&gt;</summary></entry><entry><title>"Aggregating local descriptors into a compact image representation"</title><link href="http://leohung.net/aggregating-local-descriptors-into-a-compact-image-representation.html" rel="alternate"></link><updated>2013-09-08T00:28:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:aggregating-local-descriptors-into-a-compact-image-representation.html</id><summary type="html">&lt;p&gt;Herve Jegou, et al., Aggregating local descriptors into a compact image representation, Proc. IEEE CVPR'10
The goal of the work is reducing the computing time and the memory usage without lossing too much accuracy in image retrieval for large scale data. This work consists of two parts: VLAD and vector encoding.&lt;/p&gt;
&lt;h2&gt;VLAD&lt;/h2&gt;
&lt;p&gt;Inspired by Fisher kernel, they introduce VLAD(vector of locally aggregated descriptors) to represent) to represent an image. The VLAD is processed as follows.&lt;/p&gt;
&lt;p&gt;First, train a visual words codebook&lt;/p&gt;
&lt;p&gt;&lt;img alt="c_set" src="/images/ammai/03/c_set.gif" /&gt;&lt;/p&gt;
&lt;p&gt;by K-means, where $latex c_i $ means the i'th centroid, and $latex c_i$ is a dimension d sift vector.&lt;/p&gt;
&lt;p&gt;Second, for the sift vector of each image $latex x$, whose dimension = d, aggregate the difference between $latex x$ and its nearest centroid $latex c_i $ to generate $latex v_{i,j}$ as bellow, where i means the number of centroid and j means the number of the component of sift vector. $latex v's $  dimension $latex D = k * d$.&lt;/p&gt;
&lt;p&gt;Finally, the vector v is subsequently L2-normalized by $latex v := v/||v||^2 $.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vlad" src="/images/ammai/03/vlad.png" /&gt;&lt;/p&gt;
&lt;p&gt;Approximating nearest neighbors&lt;/p&gt;
&lt;h2&gt;Indexation-aware dimensionality reduction&lt;/h2&gt;
&lt;p&gt;To make nearest neighbor search more efficiently by approximating, they proposed ADC approach, which quantize each sub-vector. They also find that VLAD is sparse and structured, which is appropriate for PCA to reduce dimensionality. To consider that PCA transformed vector's variance is not equal, it can employ Householder matrix to balance variance; besides, they find that random orthogonal matrix also performs well according to the experiment result.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;The paper contribution is proposing VLAD to represent  image and using ADC approach and PCA to compress data.  The experiment shows that the VLAD they proposed performs better than "bag of features" method. This approach can make image retrieval more efficiently and more space economically.&lt;/p&gt;</summary></entry><entry><title>"Efficient visual search of videos cast as text retrieval"</title><link href="http://leohung.net/efficient-visual-search-of-videos-cast-as-text-retrieval.html" rel="alternate"></link><updated>2013-09-08T00:23:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:efficient-visual-search-of-videos-cast-as-text-retrieval.html</id><summary type="html">&lt;p&gt;"Efficient visual search of videos cast as text retrieval," J. Sivic, and A. Zisserman, IEEE TPAMI, 2009
Paper Authors: Josef Sivic and Andrew Zisserman The following figures are all from this paper.&lt;/p&gt;
&lt;p&gt;&lt;img alt="outline" src="/images/ammai/03/outline.png" /&gt;&lt;/p&gt;
&lt;p&gt;J. Sivic, and A. Zisserman proposed this method which analogizes searching in video to in text corpora: representing a visual region as a visual word such that it can employ text retrieval methods like inverted index, tf-idf...etc. The diagram not only performs well but also saves time efficiently. The system flowchart shows as above.&lt;/p&gt;
&lt;h2&gt;Visual words&lt;/h2&gt;
&lt;p&gt;The first thing to do is to convert image regions into visual words, which are symbols can represent key image regions like word terms' effect in articles. To do so, use two affine covariant regions detecting method: Shape Adapted (SA) and Maximally Stable (MS)  to find significant regions, describe them with SIFT descriptors, cluster descriptor vectors with K-means using Mahalanbis distance as below, and transfer vectors into "visual words" according to the clustering result. Visual words not only can be represent image regions but also makes image retrieval more efficiently through inverted index method.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mahalanobis-distance" src="/images/ammai/03/Mahalanobis-distance.png" /&gt;&lt;/p&gt;
&lt;h2&gt;tf-idf weighted and Stop list&lt;/h2&gt;
&lt;p&gt;Borrowing the concept of tf-idf in text retrieval, the query vectors and retrieved frame vectors are weighted by the inverse document. And for the most frequent visual words in all most all image are suppressed, just like what Stop word list effects.&lt;/p&gt;
&lt;h2&gt;Spatial consistency&lt;/h2&gt;
&lt;p&gt;It is a accurate match not only hit one point but also the points surrounding the point are matched. Each region which also matches within the search areas, defined by the 15 nearest spatial neighbours of each match, casts a vote for that frame. Matches with no support are rejected. The ﬁnal relevance score of the frame is determined by summing the spatial consistency votes, and adding the frequency score $latex sim( v_q , v_d ) $, the consine similarity between $latex v_q$ and $latex v_d$.&lt;/p&gt;
&lt;h2&gt;My comment&lt;/h2&gt;
&lt;p&gt;This work shows how cross-fields research can be possible and why it may be more useful: the well-developed techniques in other filed can be applied if we analogizes the problem to the other appropriately. However, there are still data specific attributes, just like spatial consistence, which should be processed more sophisticatedly. .&lt;/p&gt;</summary></entry><entry><title>"Distinctive Image Features from Scale-Invariant Keypoints"</title><link href="http://leohung.net/distinctive-image-features-from-scale-invariant-keypoints.html" rel="alternate"></link><updated>2013-09-08T00:07:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-08:distinctive-image-features-from-scale-invariant-keypoints.html</id><summary type="html">&lt;p&gt;"Distinctive Image Features from Scale-Invariant Keypoints," Lowe, IJCV, 2004&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mslab.csie.ntu.edu.tw/~askus/ammai/blog/wp-content/uploads/2012/03/螢幕快照-2012-03-13-下午11.11.08.png"&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper author: David G. Lowe The following figures are all from this paper.&lt;/p&gt;
&lt;h2&gt;Novelties, contributions, assumption&lt;/h2&gt;
&lt;p&gt;This work introduce a set of promising affine invariant features, which can be used to identified specific object varied with background, illumination, rotation degree.&lt;/p&gt;
&lt;h2&gt;Questions and promising applications&lt;/h2&gt;
&lt;p&gt;Object Recognition by keypoints matching.&lt;/p&gt;
&lt;h2&gt;Technical Summarization&lt;/h2&gt;
&lt;p&gt;Scale Invariant Feature Transform (SIFT) contains two main part: finding keypoints and describing them. The process contains four steps:&lt;/p&gt;
&lt;h3&gt;Step 1&lt;/h3&gt;
&lt;p&gt;Scale-space extrema detection To find keypoints, the first thing is to detect scale-space extrema points, where are significant different from their neighbors. Koenderink (1984) and Lindeberg (1994) showed that the only possible scale-space kernel is the Gaussian function. Using the Gaussian function convolution on the image with varying scales, calculate the difference from the nearby two scales separated ny a constant factor k for finding the points whose difference is local maximum or minimum.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img1" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.32.36.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="img2" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.34.58.png" /&gt;&lt;/p&gt;
&lt;p&gt;Two reasons support this method: (a) it is a particularly efficient function to compute (b) In addition, the difference-of-Gaussian function provides a close approximation to the scale-normalized Laplacian of Gaussian, σ2∇2G, as studied by Lindeberg (1994)&lt;/p&gt;
&lt;h3&gt;Step 2. Keypoint localization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img3" src="/images/ammai/03/螢幕快照-2012-03-13-下午8.40.52.png" /&gt;&lt;/p&gt;
&lt;p&gt;In order to detect the local maxima and minima of D(x, y, σ), each sample point is compared to its eight neighbors in the current image and nine neighbors in the scale above and below (see Figure 2)Three problem left: a. k=? b. σ=? c. How many octaves?After finding keypoints, the next problem is to accurate the keypoint localization and eliminate edge responses.&lt;/p&gt;
&lt;h3&gt;Step 3 Orientation assignment&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img4" src="/images/ammai/03/螢幕快照-2012-03-13-下午10.43.34.png" /&gt;&lt;/p&gt;
&lt;p&gt;Calculate the magnitude and the orientation of the gradient of keypoints. To make the descriptor is invariant to rotation, the orientation can be represented relative to the the dominant direction.&lt;/li&gt;&lt;/p&gt;
&lt;h3&gt;Step 4 Keypoint descriptor&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img5" src="/images/ammai/03/螢幕快照-2012-03-13-下午11.11.08.png" /&gt;&lt;/p&gt;
&lt;p&gt;To describe the sample point, use m x m pixels within n x n blocks surrounding the point with k orientation bins. Sum up the orientations of m x m pixels to k bins weighted by magnitudes for each block. Therefore, one sample point would have n * n * k dimensions features vector to represent.The vector is normalized to unit length for the purpose that let the descriptor is invariant to the effects of illumination change, such as contrast, causing pixels value  multiplied with a constant factor.Besides, a non-linear illumination changes may be caused by 3D surfaces with differing orientations or  camera saturation. To cope with it, reduce the inﬂuence of large gradient magnitudes by thresholding the values in the unit feature vector to each be no larger than 0.2, and then renormalizing to unit length&lt;/p&gt;</summary></entry><entry><title>"PLSI &amp; LDA"</title><link href="http://leohung.net/plsi-lda.html" rel="alternate"></link><updated>2013-09-07T23:56:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:plsi-lda.html</id><summary type="html">&lt;p&gt;PLSI &amp;amp; LDA&lt;/p&gt;
&lt;p&gt;Origin source: "Probabilistic latent semantic indexing," T. Hofmann, SIGIR, 1999. "Latent Dirichlet allocation," D. Blei, A. Ng, and M. Jordan. . Journal of Machine Learning Research, 3:993–1022, January 2003 The following figures and formula are copied from above papers.&lt;/p&gt;
&lt;p&gt;PLSI and LDA are both hidden-class decomposition method ( or dimension reduction method ) but are different in modeling approaches; through mapping documents from high-dimension to low-dimension hidden class space, the ambiguous words relationship can be detected such that improve information retrieval precision.&lt;/p&gt;
&lt;h2&gt;PLSI&lt;/h2&gt;
&lt;p&gt;PLSI use the hidden-class decomposition technique PLSA, which is a generative probability model assuming a word in a specific document is generated as follows: (1) randomly pick a hidden-topic z for the document following P(z|d) (2) randomly pick a word w conditioning the topic z following P(w|z) (3) sum up all hidden topic result of generating word w. In other words, wordis relative to unobservable hidden-topic whose probability can not be calculated directly except an EM method, which compose with two steps: expectation step and maximization step.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pdm" src="/images/ammai/04/Pdm.png" /&gt;&lt;/p&gt;
&lt;p&gt;In E step, calculate posterior probabilities given document and word as following formula.&lt;/p&gt;
&lt;p&gt;&lt;img alt="EStep" src="/images/ammai/04/EStep.png" /&gt;&lt;/p&gt;
&lt;p&gt;In M step, calculate prior probabilities.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MStep" src="/images/ammai/04/MStep.png" /&gt;&lt;/p&gt;
&lt;p&gt;Besides, to avoid overfitting, the authors proposed TEM method, which adds a power coefficient $latex \beta $ to original EM formula as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="TEM" src="/images/ammai/04/TEM.png" /&gt;&lt;/p&gt;
&lt;h2&gt;LDA&lt;/h2&gt;
&lt;p&gt;LDA is a graphical model approach to model words generation process.&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA" src="/images/ammai/04/LDA.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;p&gt;For PLSI, the formula in the M-step minimize word perplexity directly. For LDA, because LDA is a graphical model, LDA can be extended to more complex model.&lt;/p&gt;</summary></entry><entry><title>"Nonlinear dimensionality reduction by locally linear embedding"</title><link href="http://leohung.net/nonlinear-dimensionality-reduction-by-locally-linear-embedding.html" rel="alternate"></link><updated>2013-09-07T23:49:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:nonlinear-dimensionality-reduction-by-locally-linear-embedding.html</id><summary type="html">&lt;p&gt;"Nonlinear dimensionality reduction by locally linear embedding," Roweis &amp;amp; Saul, Science, 2000.&lt;/p&gt;
&lt;p&gt;The following figures are all copied from the essay.&lt;/p&gt;
&lt;p&gt;&lt;img alt="nonlinear-dimension-reduction" src="/images/ammai/04/nonlinear-dimension-reduction.png" /&gt;&lt;/p&gt;
&lt;p&gt;The main purpose of this method is to reduce the data from high dimensions to low dimensions while maintaining instances local relative distances at the same time. There are three main steps: (1) selecting k neighbors (2) calculate contribution weights $latex W_{ij} $ (3) reducing dimensions. The algorithm can be used for not only text corpus, but also images.&lt;/p&gt;
&lt;p&gt;First, select the k nearest neighbors for each instance, where "k" is a parameter,  and distances between between neighbors will be maintain.&lt;/p&gt;
&lt;p&gt;Second, calculate the weights  $latex W_{ij} $,  the contribution of $latex j$ to $latex i$ , between the selected instance $latex i $ and its neighbors $latex j $ by minimizing the following cost function with two constraints: (1) $latex X_j $ are the neighbors of $latex X_i $. (2)&lt;/p&gt;
&lt;p&gt;&lt;img alt="constraint2" src="/images/ammai/04/constraint2.gif" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="cost-function-1" src="/images/ammai/04/cost-function-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Third, with the following cost function fixing $latex W_{ij}$ , transform instances from high-dimension X to low-dimension Y, which can be seen as a eigenvalue problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cost-function-2" src="/images/ammai/04/cost-function-2.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;p&gt;(1) Invariant Due to the cost function is a sum function, for image processing, it will be not sensitive to geometric transformation.
(2) Small amount of parameters The method use small amount of parameters, only k for selecting neighbors. ##&lt;/p&gt;
&lt;h2&gt;Disadvantages&lt;/h2&gt;
&lt;p&gt;If selecting neighbors with brute force comparison, it will consume much of time when data size raising. May be it can be solved by locality sensitive hash method to find nearest neighbors efficiently.&lt;/p&gt;</summary></entry><entry><title>"Support vector learning for ordinal regression"</title><link href="http://leohung.net/support-vector-learning-for-ordinal-regression.html" rel="alternate"></link><updated>2013-09-07T23:43:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:support-vector-learning-for-ordinal-regression.html</id><summary type="html">&lt;p&gt;"Support vector learning for ordinal regression," R. Herbrich, T. Graepel, K. Obermayer, ICANN, 1999&lt;/p&gt;
&lt;p&gt;Authors: R. Herbrich, T. Graepel, K. Obermayer
The following figures are all copies from the paper.&lt;/p&gt;
&lt;p&gt;The classical supervised machine learning methods deal with classification problem and regression problem. Instead, this research proposed a method to deal with ordinal regression, predicting the order of samples, by reducing ordinal regression problem into a classification problem on pair of object.&lt;/p&gt;
&lt;p&gt;Assume input space X ={x1, x2,...xk }, and output space  Y = {y1, y2 ...yk} where exist a order that y1 &amp;lt; y2 &amp;lt; y3 ... &amp;lt; yk. The ordinal regression problem is to find a model h mapping from X to Y such that minimize the loss function, which panelize wrong order mappings.&lt;/p&gt;
&lt;p&gt;To transfer it into classification problem, define a function p = p( h(x_1), h(x_2) ) ={-1, 0 ,+1} which means the classes of order: &amp;lt; , = , or &amp;gt;. Then, the classification problem can be transformed to find a model h that mapping from (h(x_1), h(x_2) ) to ( y_1, y_2) to minimize the loss function which is related to whether p(h(x_1),h(x_2)) is as same as p(y_1,y_2).&lt;/p&gt;
&lt;p&gt;After transformation, it can be solved by support vector machine. The experiment compared ordinal regression with multi-class SVM and support vector regression(SVR). The result showed that ordinal regression is better than others.&lt;/p&gt;
&lt;p&gt;&lt;img alt="r_measure" src="/images/ammai/05/r_measure.png" /&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, the research experimented on a IR problem, and it shows that ordinal regression performed better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IR_result" src="/images/ammai/05/IR_result.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My Opinion&lt;/h2&gt;
&lt;p&gt;According to the authors said, these work introduced a new problem in machine learning filed, and they proposed a novel and simple method to deal with ordinal regression problem. However, due to reducing process using combination of instance, I think that it will become become a big problem when the dataset grossing. When the number of instance gross  to n, the number of combination gross to O(n^2), which is hard to compute if n is large.&lt;/p&gt;</summary></entry><entry><title>"Learning to rank: from pairwise approach to listwise approach"</title><link href="http://leohung.net/learning-to-rank-from-pairwise-approach-to-listwise-approach.html" rel="alternate"></link><updated>2013-09-07T23:25:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:learning-to-rank-from-pairwise-approach-to-listwise-approach.html</id><summary type="html">&lt;p&gt;"Learning to rank: from pairwise approach to listwise approach," Cao, ICML, 2007.&lt;/p&gt;
&lt;p&gt;Authors: Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, Hang Li The following figures are all copied from this paper.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;Unlike pairwise rank-learning method like RankSVM, this work proposed a listwise rank learning framework where training instances are rank lists instead of document pairs. To predict a rank list for a query $latex q_i $, this work models the score of the rank permutation as a probability function as follows, in which $latex s_{\pi(j)} $ is a scoring function for a document in j position and \phi is a positive strict increasing function often using exponential function.&lt;/p&gt;
&lt;p&gt;&lt;img alt="prob_func" src="/images/ammai/06/prob_func.png" /&gt;&lt;/p&gt;
&lt;p&gt;The objective function of learning is to minimize the loss of list between training data and predicting data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="objective_function" src="/images/ammai/06/objective_function.png" /&gt;&lt;/p&gt;
&lt;p&gt;To reduce computation complexity, it use top K probability modeling the likelihood of top K documents order. In this research, k is selected as 1, and the scoring function is trained by Neural network optimized by gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;img alt="top-k-prob" src="/images/ammai/06/top-k-probability.png" /&gt;&lt;/p&gt;
&lt;p&gt;And the loss function will be defined as the cross entropy between training data and predicting list.&lt;/p&gt;
&lt;p&gt;&lt;img alt="top_k_loss_func" src="/images/ammai/06/top_k_loss_func.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Experiments Result&lt;/h2&gt;
&lt;p&gt;Comparing with pairwise methods: RankBoost, RankSVM and RankNet, ListNet have  better results in both MAP and NDCP measurements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="map" src="/images/ammai/06/map.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="NDCG" src="/images/ammai/06/NDCG.png" /&gt;&lt;/p&gt;
&lt;h2&gt;My Opinion&lt;/h2&gt;
&lt;p&gt;It is charming that ListNet directly model the retrieval directly based on ranking list, which is still the search engine paradigm today. However, I am curious about why the probability of permutations performs well and results with different parameters "K": beyond the consideration of computation complexity, will it work more precisely with a larget "K"?&lt;/p&gt;</summary></entry><entry><title>"A Global Geometric Framework for Nonlinear Dimensionality Reduction"</title><link href="http://leohung.net/a-global-geometric-framework-for-nonlinear-dimensionality-reduction.html" rel="alternate"></link><updated>2013-09-07T23:09:00+02:00</updated><author><name>leohung</name></author><id>tag:leohung.net,2013-09-07:a-global-geometric-framework-for-nonlinear-dimensionality-reduction.html</id><summary type="html">&lt;p&gt;Authors: Joshua B. Tenenbaum, Vin de Silva, John C. Langford&lt;/p&gt;
&lt;p&gt;The following figures are copied from the original paper.&lt;/p&gt;
&lt;p&gt;This work introduces a method called Isomap for dimension reduction specifically on non-linear data. The algorithm composes of three steps:&lt;/p&gt;
&lt;p&gt;(1) Find neighbors for each points in input space. To do so, it can be done (a) by retaining K neighbors or (b) by retaining neighbors in a constat radius range for each point. Then, represents neighbors relationship as a weighted graph, in which the weight is the distance between two points.&lt;/p&gt;
&lt;p&gt;(2) Compute shortest paths length for all pair points on the neighbors graph. Using the all pair shortest path algorithm like Floyd’s algorithm to detect all pair shortest path lengths.&lt;/p&gt;
&lt;p&gt;(3) Perform dimension reduction by applying MDS. Transfer data into low d-dimensions space Y by performing MDS on pair distances calculated in step (2). Like MDS and PCA, the true dimension d in Isomap can be estimated from error loss decreasing in transfered space Y when d increases, and also Isomap can recover true data structure if there are sufficient data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="result" src="/images/ammai/06/result-300x229.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;My Opinon&lt;/em&gt;: As the experiment result showed above, Isomap finds the structure of data well. However, just authors mentioned, it takes much data to detect the structure of a specific object, and also the time complexity is high due to the step 2 O(n^3) algorithm. I am curious that whether it is useful or not in the real application context.&lt;/p&gt;</summary></entry></feed>